\

**Denoising Medical Images**

**Using Convolutional**

**Autoencoders**

Harshad Ingole - 180050037

Sunil Kumar Meena - 180050107



\

**Denoising Medical Images**

X-Rays, MRIs, CTs, ultrasound etc are susceptible to noise. Reasons include

different image acquisition techniques to attempt at decreasing the exposure to

radiation. As the amount of radiation decreases, the noise level increases.

Denoising is very necessary for proper analysis, both by humans and machines.

Denoising is a classical problem given as:

z=x+y; where z is the noisy image, x is the original image and y is the noise

assumed to be generated by a well defined process. The goal is to predict x using

z.



\

**Denoising using Deep Learning**

Many deep learning models have shown promising results in denoising. We have

decided to use a **Convolutional Autoencoder **for the task of Denoising.

Autoencoders are artificial NN that learn efficient data coding **by ignoring the**

**noise**.

Convolutional layers have the ability to **exploit strong spatial correlations **in

images..

An autoencoder with convolutional layers - a Convolutional Autoencoder - thus

easily outperforms traditional denoising methods like median and mean filtering.



\

**Autoencoders**

Widely used for dimensionality reduction and

feature learning, autoencoders first **encodes**

the input thereby learning a low dimensional

representation. Much like PCA. Then it

**decodes **or upsamples the coded data to

give the output.

Basically, it tries to learn an approximation of

the identity function but in the process

eliminates the “noise”.



\

**Convolutional Autoencoders**

Convolutional Autoencoders are based on standard autoencoder architecture but

have convolutional encoding and decoding layers. They utilize the full capability of

CNN to exploit image structure.

The ith feature map is given as : hi=s(x∗Wi+bi) where ∗ is the convolution operation

and s is an activation.



\

**Architecture**

The architecture is fairly simple. It consists of 2

encoding convolutional layer. Each consisting of

2D conv followed by maxpooling.

It is followed by two decoding layers. Each

consists of upsampling followed by conv.

We were planning on having a deeper

architecture with skip symmetric connections.



\

**Architecture**



\

**Evaluation**

Used 2 datasets - mini-MIAS dataset of

mammograms, and dental radiography database.

Added Gaussian noise to the datasets and trained on 60% of the dataset.

Used binary cross entropy loss function. Ran for 10 epochs with a batch size of 10.



\

**Evaluation**

Images were compared using

Structural Similarity Index (SSIM) .

For original and coded signal x

and y :



\

**Results - **μ =0, σ= 1, p= 0.1

We could replicate the results in the paper:

Image type

Noisy

Mammograms

0.44

Dental

0.64

CNN DAE

Median Filter

0.82

0.87

0.73

0.85



\

**Results - Mammograms**



\

**Results - Dental**



\

**Results**

For low noise levels, our model performs better that the traditional method of

median filtering

But this is only for low noise levels. The model has a tough time converging at

higher noise levels.



\

**Conclusion**

While the model does better that median filtering for lower noise levels, the image

results are not that impressive. We believe that a deeper architecture will perform

better.

There is another paper with a deeper architecture and it uses skip symmetric

connections for faster convergence.

So while Autoencoders may not be SOTA for denoising, they do provide a feasible

low computational cost method for denoising low noise level images.


